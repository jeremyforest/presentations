{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Cascading complexity framework\"\n",
        "author: \"Jeremy Forest\"\n",
        "date: 'November 22, 2022'\n",
        "format: \n",
        "  revealjs:\n",
        "    incremental: true\n",
        "    fontsize: 18pt\n",
        "    smaller: false\n",
        "    scrollable: true\t\t\n",
        "    theme: dark\n",
        "    slide-number: true\n",
        "    show-slide-number: all\n",
        "    preview-links: auto\n",
        "    chalkboard: true\n",
        "    self-contained: true\n",
        "revealjs-plugins:\n",
        "  - pointer\n",
        "toc: false\n",
        "number-sections: false\n",
        "#bibliography: references.bib\n",
        "#jupyter: python3\n",
        "---"
      ],
      "id": "1335549d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction\n",
        "\n",
        "## Introduction\n",
        "\n",
        "::: {layout-ncol=2}\n",
        "- Simple neuronal model have limits and don't fit perfectly biological data, although arguably some come close at least on some measures.  \n",
        "\n",
        "::: {.fragment}\n",
        "![](/images/Neuromatch_GIFaccuracy.png)\n",
        ":::\n",
        ":::\n",
        "\n",
        "- Those limits can be the reason we struggle to have models performs specific taqsks \n",
        "    \n",
        "  &rarr; e.g. continual learning and the catastrophic forgetting problem.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "::: {layout-ncol=2}\n",
        "- The typical unit model in ANN is the point-neuron.\n",
        "\n",
        "- In SNN typical units are more varied but most come down to LIF and LIF derivatives.\n",
        "\n",
        "::: {.fragment}\n",
        "![](/images/Deng2020_ANNandSNN.png){fig-align=\"center\"}\n",
        ":::\n",
        ":::\n",
        "\n",
        "- In both case lots of the complexity is abstracted away for better computational efficiency.\n",
        "\n",
        ":::{#title-slide .center}\n",
        "- ***Which details matter and which don't ?***\n",
        ":::\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "- Recent litterature point toward the importance of complexity both at the level of single neuron and in neural networks. \n",
        "\n",
        "- Examples: \n",
        "  \n",
        "  + Payer et al. 2020 :\n",
        "  \n",
        "    > we demonstrate that, when paired with short-term synaptic dynamics, regenerative activity in the apical dendrites, and synaptic plasticity in feedback pathways, a burst-dependent learning rule can solve challenging tasks that require deep network architectures. Our results demonstrate that well-known properties of dendrites, synapses, and synaptic plasticity are sufficient to enable sophisticated learning in hierarchical circuits.\n",
        "\n",
        "  + Perez-Nieves et al. 2020: \n",
        "    \n",
        "    > the performance of spiking neural networks trained to carry out tasks of real-world difficulty, with varying degrees of heterogeneity, and found that it substantially improved task performance.\n",
        "\n",
        "  + Zeldenrust et al. 2021: \n",
        "  \n",
        "    >networks consisting of a heterogeneous population of different neuron types are both more efficient and more robust against correlated noise.\n",
        "\n",
        "  + Jones et al. 2021:\n",
        "\n",
        "    > we use a simple model where the dendrite is implemented as a sequence of thresholded linear units. We manipulate the architecture of this model to investigate the impacts of binary branching constraints and repetition of synaptic inputs on neural computation. We find that models with such manipulations can perform well on machine learning tasks, such as Fashion MNIST or Extended MNIST. We find that model performance on these tasks is limited by binary tree branching and dendritic asymmetry and is improved by the repetition of synaptic inputs to different dendritic branches.\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "::: {layout-ncol=2}\n",
        "- Can we go toward more detailed models while still make efforts toward bare-bone complexity, keeping only the essentials ?\n",
        "\n",
        "::: {.fragment}\n",
        "![](/images/models.drawio.png){fig-align=\"center\"}\n",
        ":::\n",
        ":::\n",
        "\n",
        "- We want to build a simple enough neuron model that \n",
        "  \n",
        "  + capture more of the biological complexity based on the hypothesis that those units/networks will perform better on some metrics/tasks\n",
        "\n",
        "  + while remaining as efficient as possible so we can still simulate complex networks.\n",
        "\n",
        "- We are going to look at 2 elements here:\n",
        "\n",
        "  + dendrites\n",
        "  \n",
        "  + synapses\n",
        "\n",
        "\n",
        "# Dendrites\n",
        "<!--#  \n",
        "- Active dendrite work (others and mine with Numenta), Poirazi work and dendrify, application in loihi2 and neuromorphics\n",
        "- routing relevant information and coincidence detector (Numenta and me, poirazi et al)\n",
        "- importance of sparsity\n",
        "- growing away from point neurons _> dendrite as a new base unit\n",
        "- potential ability to track credut assigment -> Blake richards work\n",
        "- dendrify and setting up more complex variaiton of dendrify through sapinet and application to neuromorphics for information routing and sparsely\n",
        "- pattern completion vs separation in memory, recruit same dendrites. Prime same spines (ref?) coincidence detection. Intrinsic activity level. \n",
        "-->\n",
        "\n",
        "## Conceptual usefullness of dendrites\n",
        "\n",
        "- Larkum 2022: Are dendrites conceptually useful?\n",
        "\n",
        "  > Their function is usually described as “integrate-and-fire” where linearly summated weighted synaptic inputs determine, via an activation function, if the neuron should be ‘on’ or ‘off’. It could be argued that this conception of a neuron collapses the role of dendrites in to the strength of synaptic inputs thus relegating their function to a mere after-thought, and therefore not conceptually useful. There is a long history of disregarding dendrites [...] and the relatively brief renaissance of interest in dendrites [...] has more recently been challenged by the incredible success of deep neural networks with point neurons. Indeed, neuroscience now turns to deep neural networks for insights into brain function [...] suggesting that neural networks might already use the optimal abstraction of a neuron. The burden of proof has once again shifted to the biologists to demonstrate that the point-neuron abstraction is conceptually lacking.\n",
        "\n",
        "::: {layout-ncol=2}\n",
        "- Beniaguev et al. 2021 showed that a layer 5 pyramidal neuron I/O could be approximated using a 7 layers ANN composed of point neurons.\n",
        "\n",
        "::: {.fragment}\n",
        "![](/images/Beniaguev2021_L5pyras7layerANN.png)\n",
        ":::\n",
        ":::\n",
        "\n",
        ":::{#title-slide .center}\n",
        "- ***Is there a point looking at dendrites ?*** \n",
        "::: \n",
        "\n",
        "\n",
        "## Dendrites : coincidence detectors and information routing\n",
        "\n",
        "- Dendrite can act as coincidence detectors.\n",
        "\n",
        "- Differing information content is routed to different part of the dendritic arbor\n",
        "\n",
        "::: {layout-ncol=2}\n",
        "::: {.fragment}\n",
        "![](/images/Spruston2008_coincidencedetection.png)\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "![](/images/poirazi2022talk_dendriterouting.png)\n",
        "\n",
        "![](/images/major2012_dendritesrouting.png)\n",
        ":::\n",
        "\n",
        ":::\n",
        "\n",
        "## Linear and non-linear computations\n",
        "<!--# Lots of this is inspired by P.Poirazi talk at snufa 2022 https://www.youtube.com/watch?v=T3s-R4vXtZs&list=PL09WqqDbQWHFjkkXiQdOYC1wAdgSUMGxQ&index=7-->\n",
        "\n",
        "- Neurons contains active or passive dendrites. \n",
        "\n",
        "- Passive dendrites show only linear or sublinear input summation.\n",
        "\n",
        "- Active dendrite can show supralinear outpout. \n",
        "\n",
        "::: {.fragment}\n",
        "Same dendritic branch stimulation can results in linear or supra-linear output depending on stimulation conditions.\n",
        "\n",
        "![](/images/Ariav2003_dendrites.png){.absolute top=300 right=800 width=\"200\" height=\"400\"}\n",
        "\n",
        "![](/images/Ariav2003_spatialsummation.png){.absolute top=300 right=400 width=\"350\" height=\"300\"}\n",
        "\n",
        "![](/images/Ariav2003_temporalsummationwindow.png){.absolute top=300 right=0 width=\"350\" height=\"300\"}\n",
        ":::\n",
        "\n",
        "## Between dendrites VS within dendrite summation {.smaller}\n",
        "::: {.r-fit-text}\n",
        "Same dendrite stimulation can result in non-linearity whereas that is not the case for between dendrites stimulations\n",
        ":::\n",
        "\n",
        "![](/images/London2005_betweenVSwithin.png)\n",
        "\n",
        "\n",
        "## Dendritic plateau potentials\n",
        "\n",
        "::: {layout-ncol=2}\n",
        "![](/images/Poirazi2020_dspike.png)\n",
        "\n",
        "![](/images/Antic2010_dspikes.png)\n",
        ":::\n",
        "\n",
        "## Dendrite hetereogeneity\n",
        "\n",
        "Dendrites functional properties depends on their morphology hence lots of room for variability / hetereogeneity in dendrites properties. \n",
        "\n",
        "![](/images/Tran-Van-Minh2015_dendritehetereogeneity.png)\n",
        "\n",
        "\n",
        "## Dendrite models \n",
        "\n",
        "Example of dendrites mathematical abstractions:\n",
        "\n",
        "![](/images/Poirazi2020_dendriteneuron.png){.absolute top=200 left=0 width=\"400\" height=\"400\"}\n",
        "![](/images/Poirazi2020_deepdendriteANN.png){.absolute top=200 right=200 width=\"400\" height=\"400\"}\n",
        "\n",
        "\n",
        "## Dendrites relevance to neuronal computation\n",
        "\n",
        "Case study 1: Tzilivaki et al 2019\n",
        "\n",
        "- Non linear dendrites in interneurons help store memories using fewer ressources\n",
        "- Linked memory are stored in common dendrites, through synapse clustering\n",
        "\n",
        "::: {layout=\"[[50, 15, 35]]\"}\n",
        "::: {.fragment}\n",
        "![](/images/Tzilivaki2019_dendritememorytask.png)\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "![](/images/Sehgal2021_dendriteoverlap.png)\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "![](/images/Sehgal2021_spineaddedondendrite.png)\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Dendrites relevance to neuronal computation\n",
        "\n",
        "Case study 2: Grewal et al 2021 and Iyer et al. 2022\n",
        "\n",
        "- Adding dendrite abstractions in an MLP help preventing catastrophic forgetting both in abstract task (permutedMNIST) and RL tasks.\n",
        "\n",
        "- Dendrite specialize in few tasks. \n",
        "\n",
        "\n",
        "::: {layout=\"[[50, 50], [50, 50]]\"}\n",
        "::: {.fragment}\n",
        "![](/images/Iyer2022_model.png)\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "![](/images/Grewal2021_dendritemodelaccuracy.png)\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "![](/images/Iyer2022_dendritespecialization.png)\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "![](/images/Iyer2022_RLperf.png)\n",
        ":::\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Synapses\n",
        "<!--#  \n",
        "- Stefano work and a bit of historical background on it, relation to loihi2 and grade synapses.\n",
        "- memory capacity\n",
        "-->\n",
        "\n",
        "## Synapse modelling\n",
        "\n",
        "- Varied synaptic weight (integer) by minimizing a cost function (typical deep learning). \n",
        "\n",
        "  + Not bio-plausible (among other things)\n",
        "\n",
        "- Switch-like fashion ON/OFF (Hopfield) \n",
        "\n",
        "  + Doesn't fit memory data i.e. power law decay of memories\n",
        "  \n",
        "  + Retrieving memories difficult after some time\n",
        "\n",
        "  + Memory capacity limited by network size and prompt to interference\n",
        "\n",
        "- STDP, LTP, LTD\n",
        "\n",
        "  + Good but\n",
        "\n",
        "  + Not enough on its own\n",
        "\n",
        "  + Ongoing activity erase modifications\n",
        "\n",
        "\n",
        "## Complex synapse\n",
        "\n",
        "- An effort to prevent overwriting of synapses previous activity.\n",
        "\n",
        "- Combine dynamical processes that store memories in bidirectional fast and slow variables. \n",
        "\n",
        "::: {layout=\"[[50,50], [50,50]]\"}\n",
        "\n",
        "::: {.fragment}\n",
        "![](/images/Fusi2016_model.png)\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "![](/images/Fusi2016_synapticnetwork.png)\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "![](/images/Fusi2016_modelresults.png)\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "![](/images/Fusi2016_memoryresults.png)\n",
        ":::\n",
        "\n",
        ":::\n",
        "\n",
        "## Complex synapse\n",
        "\n",
        "- In this approach results degrade significantly when encoded memories are correlated in time and space. But since dendrites route information and segregate information on different dendrites, not sure that hold with dendrites.  \n",
        "\n",
        "- Example: Iyer et al. 2022\n",
        "\n",
        "::: {.fragment}\n",
        "![](/images/Iyer2022_sianddendrites.png)\n",
        ":::\n",
        "\n",
        "\n",
        "## Complex synapse\n",
        "Another framework: extension of the Thomson-Gunawardena framework\n",
        "\n",
        "- Feliu et al. 2010 A General Mathematical Framework Suitable for Studying Signaling Cascades\n",
        "\n",
        "- For signaling cascades, we show that the number of equations reduces to (at most) M + n − 1, where n is the length of the cascade and M the number of free enzymes (non-substrate enzymes).\n",
        "\n",
        "::: {.fragment}\n",
        "![](/images/Feliu2010_enzymecascade.png)\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "![](/images/Feliu2010_enzymecascadeeq1.png)\n",
        "![](/images/Feliu2010_enzymecascadeeq2.png)\n",
        "![](/images/Feliu2010_enzymecascadeeq3.png)\n",
        ":::\n",
        "\n",
        "# Hierarchical multi-dynamical implementation of information storage (cascading complexity framework)\n",
        "\n",
        "- Information storage (i.e. memory) at multitemporal and multispacial scales by combining complex synapses, dendrites and neurons' intrinsic activation state.\n",
        "\n",
        "- General framework of cascading complexity from neuronal variables (i.e molecules or ensemble of molecule involved in phosphorylation cascades that underly the encoding process) to engram ensemble. \n",
        "\n",
        "- Memory capacity of the system is dependent in the hierarchical multi-dynamic implementation which itself is orthogonal to the nature of the encoded signal but englobed within the same systemic components.\n",
        "\n",
        "\n",
        "## Elements\n",
        "\n",
        "- synapses\n",
        "\n",
        "  + complex synapse containing multiple hidden state variables\n",
        "  + state variable number is hetereogenous\n",
        "\n",
        "- dendrites\n",
        "\n",
        "  + passive or active\n",
        "  + hetereogenous non-linearity properties\n",
        "\n",
        "- neurons\n",
        "\n",
        "  + intrinsic activity level defined by past activation\n",
        "  + standard LIF hetereogeneity\n",
        "\n",
        "::: {.fragment}\n",
        "\n",
        "```{dot}\n",
        "digraph D {\n",
        "  \n",
        "  node [shape=oval];\n",
        "\n",
        "  in        [label = \"Input or presynaptic neuron)\"];\n",
        "  neuron1   [label = \"NEURON 1\", style=filled, fillcolor = \"magenta\"];\n",
        "  syn1      [label = \"Syn1\"];\n",
        "  syn2      [label = \"Syn2\"];\n",
        "  syn3      [label = \"Syn3\"];\n",
        "  dend1     [label = \"Dend1 (active)\"];\n",
        "  dend2     [label = \"Dend2 (active)\"];\n",
        "  dend3     [label = \"Dend3 (passive)\"];\n",
        "  s1_1      [label = \"V1\", style=filled, fillcolor = \"grey\"];\n",
        "  s1_2      [label = \"V2\"];\n",
        "  s2_1      [label = \"V1\"];\n",
        "  s2_2      [label = \"V2\"];\n",
        "  s2_3      [label = \"V3\", style=filled, fillcolor = \"grey\"];\n",
        "  s2_4      [label = \"V4\"];\n",
        "  s3_1      [label = \"V1\"];\n",
        "  s3_2      [label = \"V2\", style=filled, fillcolor = \"grey\"];\n",
        "  s3_3      [label = \"V3\"];\n",
        "  s4_1      [label = \"V1\"];\n",
        "  s4_2      [label = \"V2\", style=filled, fillcolor = \"grey\"];\n",
        "  in_LIF    [label = \"LIF equation\"]\n",
        "  out       [label = \"Output\"];\n",
        " \n",
        "  in -> syn1;\n",
        "  in -> syn2;\n",
        "  in -> syn3;\n",
        "  in -> syn4;\n",
        "  syn1 -> dend1\n",
        "  syn2 -> dend1\n",
        "  syn3 -> dend2\n",
        "  syn4 -> dend3\n",
        "  dend1 -> in_LIF\n",
        "  dend2 -> in_LIF\n",
        "  dend3 -> in_LIF\n",
        "  in_LIF -> out\n",
        "\n",
        "  subgraph cluster_neuron1 {\n",
        "    neuron1\n",
        "\n",
        "    subgraph cluster_syn1 {\n",
        "      {rank=same s1_1 s1_2 syn1}\n",
        "      s1_1 -> s1_2\n",
        "      s1_2 -> s1_1\n",
        "    }\n",
        "    subgraph cluster_syn2 {\n",
        "      {rank=same s2_1 s2_2 s2_3 s2_4 syn2}\n",
        "      s2_1 -> s2_2 -> s2_3 -> s2_4 \n",
        "      s2_4 -> s2_3 -> s2_2 -> s2_1\n",
        "    }\n",
        "    subgraph cluster_syn3 {\n",
        "      {rank=same s3_1 s3_2 s3_3 syn3}\n",
        "      s3_1 -> s3_2 -> s3_3\n",
        "      s3_3 -> s3_2 -> s3_1\n",
        "    }\n",
        "    subgraph cluster_syn4 {\n",
        "      {rank=same s4_1 s4_2 syn4}\n",
        "      s4_1 -> s4_2\n",
        "      s4_2 -> s4_1\n",
        "    }\n",
        "\n",
        "    dend1;\n",
        "    dend2;\n",
        "    dend3;\n",
        "    in_LIF;\n",
        "\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        ":::\n",
        "\n",
        "## Dendrites desiredata\n",
        "<!-- \n",
        "\n",
        "```{dot}\n",
        "\n",
        "  digraph D {\n",
        "    node [shape=oval];\n",
        "\n",
        "    input_type1     [label = \"input type 1\"];\n",
        "    input_type2     [label = \"input type 2\"];\n",
        "\n",
        "    neuron1         [label = \"Neuron_1\", style=filled, fillcolor = \"magenta\"];\n",
        "    neuron2         [label = \"Neuron_2\", style=filled, fillcolor = \"magenta\"];\n",
        "    soma            [label = \"Soma_1\"]\n",
        "    syn1            [label = \"Syn1_1\"];\n",
        "    syn2            [label = \"Syn1_2\"];\n",
        "    syn3            [label = \"Syn1_3\"];\n",
        "    dend0           [label = \"Dend0_0a == basal\"];\n",
        "    dend1           [label = \"Dend1_0a\"];\n",
        "    dend2           [label = \"Dend1_1a\"];\n",
        "    dend3           [label = \"Dend1_1b\"];\n",
        "\n",
        "\n",
        "    \n",
        "    input_type1 -> dend0\n",
        "    input_type2 -> dend2\n",
        "    input_type2 -> dend3\n",
        "\n",
        "\n",
        "    subgraph neuron_1 {\n",
        "      neuron1\n",
        "\n",
        "    subgraph dendritic_organization_1 {\n",
        "      dend0 -> soma\n",
        "      soma -> dend1\n",
        "      dend1 -> dend2\n",
        "      dend1 -> dend3\n",
        "\n",
        "    subgraph synaptic_organisation_1{\n",
        "      dend2 -> syn1\n",
        "      dend2 -> syn2\n",
        "    \n",
        "    subgraph synaptic_organisation_2{\n",
        "      dend3 -> syn3\n",
        "    }}}}\n",
        "\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "``` \n",
        "\n",
        "-->\n",
        "\n",
        "\n",
        "\n",
        "## Simulation and Theory\n"
      ],
      "id": "582df2bb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn import Module\n",
        "import networkx as nx\n",
        "from networkx import DiGraph\n",
        "\n",
        "class Component(Module):\n",
        "    def __init__(self, name, type):\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "        self.type = type\n",
        "\n",
        "    def forward(self, data):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class GenericConnector(Component):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.type = 'generic_connector'\n",
        "        self.src = src \n",
        "        self.dst = dst\n",
        "\n",
        "class Soma(Component):\n",
        "    def __init__(self, name):\n",
        "        self.type = \"soma\"\n",
        "        self.name = name\n",
        "        super().__init__()\n",
        "\n",
        "class Dendrite(Component):\n",
        "    def __init__(self, name):\n",
        "        super().__init__()\n",
        "        self.type = \"dendrite\"\n",
        "      \n",
        "class Synapse(GenericConnector):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.type = 'synapse'\n",
        "\n",
        "class Network(Module):\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.graph = DiGraph()    \n",
        "\n",
        "    def add_soma(self, soma):\n",
        "        self.graph.add_node(soma.name)\n",
        "\n",
        "    def add_dendrite(self, dendrite):\n",
        "        self.graph.add_node(dendrite.name)\n",
        "\n",
        "    def add_synapse(self, synapse):\n",
        "        self.graph.add_edge(synapse.src, synapse.dst)\n",
        "\n",
        "    def add_generic_connector(self, genericConnector):\n",
        "        self.graph.add_edge(genericConnector.src, genericConnector.dst)\n",
        "\n",
        "\n",
        "# create a new graph\n",
        "net = Network(name='net')\n",
        "\n",
        "# add nodes to represent the neurons and dendrites\n",
        "soma = Soma('sama')\n",
        "\n",
        "# Let's use the dendritic branching order as reference\n",
        "distal = Dendrite('distal')\n",
        "proximal = Dendrite('proximal')\n",
        "trunk = Dendrite('trunk')\n",
        "# Connect the dendrites with other dendrites with edge that just forward the info without modifying it\n",
        "distal_to_proximal = GenericConnector(name='distal_to_proximal', src=distal, dst=proximal) \n",
        "proximal_to_trunk = GenericConnector(name='proximal_to_trunk', src=proximal, dst=trunk) \n",
        "trunk_to_soma = GenericConnector(name='trunk_to_soma', src=trunk, dst=soma) \n",
        "\n",
        "\n",
        "# Building network flow\n",
        "net.add_soma('soma')\n",
        "\n",
        "net.add_dendrite(distal)\n",
        "net.add_dendrite(proximal)\n",
        "net.add_dendrite(trunk)\n",
        "\n",
        "net.add_generic_connector(distal_to_proximal)\n",
        "net.add_generic_connector(proximal_to_trunk)\n",
        "net.add_generic_connector(trunk_to_soma)\n",
        "\n",
        "print(net)\n",
        "\n",
        "# plot the graph\n",
        "nx.draw(net.graph, with_labels=True)\n",
        "plt.show()"
      ],
      "id": "6736f270",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Porting to neuromorphic architecture (loihi2)\n",
        "\n",
        "to come\n",
        "\n",
        "\n",
        "## Thank you {.center}"
      ],
      "id": "b5a494e5"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}