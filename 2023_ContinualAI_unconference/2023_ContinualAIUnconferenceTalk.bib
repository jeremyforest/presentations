
@article{jones_might_2021,
	title = {Might a {Single} {Neuron} {Solve} {Interesting} {Machine} {Learning} {Problems} {Through} {Successive} {Computations} on {Its} {Dendritic} {Tree}?},
	volume = {33},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco_a_01390},
	doi = {10.1162/neco_a_01390},
	abstract = {Physiological experiments have highlighted how the dendrites of biological neurons can nonlinearly process distributed synaptic inputs. However, it is unclear how aspects of a dendritic tree, such as its branched morphology or its repetition of presynaptic inputs, determine neural computation beyond this apparent nonlinearity. Here we use a simple model where the dendrite is implemented as a sequence of thresholded linear units. We manipulate the architecture of this model to investigate the impacts of binary branching constraints and repetition of synaptic inputs on neural computation. We find that models with such manipulations can perform well on machine learning tasks, such as Fashion MNIST or Extended MNIST. We find that model performance on these tasks is limited by binary tree branching and dendritic asymmetry and is improved by the repetition of synaptic inputs to different dendritic branches. These computational experiments further neuroscience theory on how different dendritic properties might determine neural computation of clearly defined tasks.},
	number = {6},
	urldate = {2021-05-27},
	journal = {Neural Computation},
	author = {Jones, Ilenna Simone and Kording, Konrad Paul},
	month = may,
	year = {2021},
	pages = {1554--1571},
	file = {Full Text PDF:/home/jeremy/Documents/Zotero_backup/storage/4MX4FGZ8/Jones and Kording - 2021 - Might a Single Neuron Solve Interesting Machine Le.pdf:application/pdf},
}

@article{london_dendritic_2005,
	title = {{DENDRITIC} {COMPUTATION}},
	abstract = {One of the central questions in neuroscience is how particular tasks, or computations, are implemented by neural networks to generate behavior. The prevailing view has been that information processing in neural networks results primarily from the properties of synapses and the connectivity of neurons within the network, with the intrinsic excitability of single neurons playing a lesser role. As a consequence, the contribution of single neurons to computation in the brain has long been underestimated. Here we review recent work showing that neuronal dendrites exhibit a range of linear and nonlinear mechanisms that allow them to implement elementary computations. We discuss why these dendritic properties may be essential for the computations performed by the neuron and the network and provide theoretical and experimental examples to support this view.},
	language = {en},
	author = {London, Michael and Häusser, Michael},
	year = {2005},
	pages = {32},
	file = {London and Häusser - 2005 - DENDRITIC COMPUTATION.pdf:/home/jeremy/Documents/Zotero_backup/storage/D8LMRZBF/London and Häusser - 2005 - DENDRITIC COMPUTATION.pdf:application/pdf},
}

@article{larkum_are_2022,
	title = {Are dendrites conceptually useful?},
	issn = {0306-4522},
	url = {https://www.sciencedirect.com/science/article/pii/S0306452222001208},
	doi = {10.1016/j.neuroscience.2022.03.008},
	abstract = {This article presents the argument that, while understanding the brain will require a multi-level approach, there is nevertheless something fundamental about understanding the components of the brain. I argue here that the standard description of neurons is not merely too simplistic, but also misses the true nature of how they operate at the computational level. In particular, the humble point neuron, devoid of dendrites with their powerful computational properties, prevents conceptual progress at higher levels of understanding.},
	language = {en},
	urldate = {2022-03-21},
	journal = {Neuroscience},
	author = {Larkum, Matthew},
	month = mar,
	year = {2022},
	file = {Larkum - 2022 - Are dendrites conceptually useful.pdf:/home/jeremy/Documents/Zotero_backup/storage/RNVZAAA9/Larkum - 2022 - Are dendrites conceptually useful.pdf:application/pdf;ScienceDirect Snapshot:/home/jeremy/Documents/Zotero_backup/storage/6VVFAXXU/S0306452222001208.html:text/html},
}

@techreport{grewal_going_2021,
	title = {Going {Beyond} the {Point} {Neuron}: {Active} {Dendrites} and {Sparse} {Representations} for {Continual} {Learning}},
	copyright = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	shorttitle = {Going {Beyond} the {Point} {Neuron}},
	url = {https://www.biorxiv.org/content/10.1101/2021.10.25.465651v1},
	abstract = {{\textless}p{\textgreater}Biological neurons integrate their inputs on dendrites using a diverse range of non-linear functions. However the majority of artificial neural networks (ANNs) ignore biological neurons9 structural complexity and instead use simplified point neurons. Can dendritic properties add value to ANNs? In this paper we investigate this question in the context of continual learning, an area where ANNs suffer from catastrophic forgetting (i.e., ANNs are unable to learn new information without erasing what they previously learned). We propose that dendritic properties can help neurons learn context-specific patterns and invoke highly sparse context-specific subnetworks. Within a continual learning scenario, these task-specific subnetworks interfere minimally with each other and, as a result, the network remembers previous tasks significantly better than standard ANNs. We then show that by combining dendritic networks with Synaptic Intelligence (a biologically motivated method for complex weights) we can achieve significant resilience to catastrophic forgetting, more than either technique can achieve on its own. Our neuron model is directly inspired by the biophysics of sustained depolarization following dendritic NMDA spikes. Our research sheds light on how biological properties of neurons can be used to solve scenarios that are typically impossible for traditional ANNs to solve.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2021-10-26},
	author = {Grewal, Karan and Forest, Jeremy and Cohen, Ben and Ahmad, Subutai},
	month = oct,
	year = {2021},
	doi = {10.1101/2021.10.25.465651},
	note = {Company: Cold Spring Harbor Laboratory
Distributor: Cold Spring Harbor Laboratory
Label: Cold Spring Harbor Laboratory
Section: New Results
Type: article},
	keywords = {Lu},
	pages = {2021.10.25.465651},
	file = {Grewal et al_2021_Going Beyond the Point Neuron.pdf:/home/jeremy/Documents/Zotero_backup/storage/65DMJ3LZ/Grewal et al_2021_Going Beyond the Point Neuron.pdf:application/pdf;Snapshot:/home/jeremy/Documents/Zotero_backup/storage/MA9BDLL5/2021.10.25.465651v1.html:text/html},
}

@article{antic_decade_2010,
	title = {The decade of the dendritic {NMDA} spike},
	volume = {88},
	doi = {10.1002/jnr.22444},
	journal = {Journal of Neuroscience Research},
	author = {Antic, Srdjan D. and Zhou, Wen-Liang and Moore, Anna R. and Short, Shaina M. and Ikonomu, Katerina D.},
	year = {2010},
	pages = {2991--3001},
	file = {Antic et al_2010_The decade of the dendritic NMDA spike.pdf:/home/jeremy/Documents/Zotero_backup/storage/BNGXETMZ/Antic et al_2010_The decade of the dendritic NMDA spike.pdf:application/pdf},
}

@article{spruston_pyramidal_2008,
	title = {Pyramidal neurons: dendritic structure and synaptic integration.},
	volume = {9},
	issn = {1471-003X},
	doi = {10.1038/nrn2286},
	journal = {Nature reviews. Neuroscience},
	author = {Spruston, Nelson},
	year = {2008},
	pmid = {18270515},
	note = {ISBN: 1471-0048},
	pages = {206--221},
	file = {Spruston_2008_Pyramidal neurons.pdf:/home/jeremy/Documents/Zotero_backup/storage/VC6LZJ3F/Spruston_2008_Pyramidal neurons.pdf:application/pdf},
}

@misc{sehgal_co-allocation_2021,
	title = {Co-allocation to overlapping dendritic branches in the retrosplenial cortex integrates memories across time},
	copyright = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2021.10.28.466343v2},
	doi = {10.1101/2021.10.28.466343},
	abstract = {Events occurring close in time are often linked in memory, providing an episodic timeline and a framework for those memories. Recent studies suggest that memories acquired close in time are encoded by overlapping neuronal ensembles, but whether dendritic plasticity plays a role in linking memories is unknown. Using activity-dependent labeling and manipulation, as well as longitudinal one- and two-photon imaging of RSC somatic and dendritic compartments, we show that memory linking is not only dependent on ensemble overlap in the retrosplenial cortex, but also on branch-specific dendritic allocation mechanisms. These results demonstrate a causal role for dendritic mechanisms in memory integration and reveal a novel set of rules that govern how linked, and independent memories are allocated to dendritic compartments.
One-Sentence Summary Dendritic allocation mechanisms link distinct memories across time},
	language = {en},
	urldate = {2022-12-13},
	publisher = {bioRxiv},
	author = {Sehgal, Megha and Filho, Daniel Almeida and Kastellakis, George and Kim, Sungsoo and Lee, Jinsu and Martin, Sunaina and Mejia, Irene Davila and Pekcan, Asli and Huang, Shan and Lavi, Ayal and Heo, Won Do and Poirazi, Panayiota and Trachtenberg, Joshua T. and Silva, Alcino J.},
	month = dec,
	year = {2021},
	note = {Pages: 2021.10.28.466343
Section: New Results},
	file = {Full Text PDF:/home/jeremy/Documents/Zotero_backup/storage/WCWLVHWJ/Sehgal et al. - 2021 - Co-allocation to overlapping dendritic branches in.pdf:application/pdf},
}

@article{tzilivaki_challenging_2019,
	title = {Challenging the point neuron dogma: {FS} basket cells as 2-stage nonlinear integrators},
	volume = {10},
	copyright = {2019 The Author(s)},
	issn = {2041-1723},
	shorttitle = {Challenging the point neuron dogma},
	url = {https://www.nature.com/articles/s41467-019-11537-7},
	doi = {10.1038/s41467-019-11537-7},
	abstract = {Interneurons are critical for the proper functioning of neural circuits. While often morphologically complex, their dendrites have been ignored for decades, treating them as linear point neurons. Exciting new findings reveal complex, non-linear dendritic computations that call for a new theory of interneuron arithmetic. Using detailed biophysical models, we predict that dendrites of FS basket cells in both hippocampus and prefrontal cortex come in two flavors: supralinear, supporting local sodium spikes within large-volume branches and sublinear, in small-volume branches. Synaptic activation of varying sets of these dendrites leads to somatic firing variability that cannot be fully explained by the point neuron reduction. Instead, a 2-stage artificial neural network (ANN), with sub- and supralinear hidden nodes, captures most of the variance. Reduced neuronal circuit modeling suggest that this bi-modal, 2-stage integration in FS basket cells confers substantial resource savings in memory encoding as well as the linking of memories across time.},
	language = {en},
	number = {1},
	urldate = {2022-12-13},
	journal = {Nature Communications},
	author = {Tzilivaki, Alexandra and Kastellakis, George and Poirazi, Panayiota},
	month = aug,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Biophysical models, Learning algorithms, Long-term memory, Network models},
	pages = {3664},
	file = {Challenging the point neuron dogma\: FS basket cells as 2-stage nonlinear integrators:/home/jeremy/Documents/Zotero_backup/storage/TUTB8835/10.1038@s41467-019-11537-7.pdf.pdf:application/pdf;Full Text PDF:/home/jeremy/Documents/Zotero_backup/storage/FCX54BJA/Tzilivaki et al. - 2019 - Challenging the point neuron dogma FS basket cell.pdf:application/pdf},
}

@article{major_active_2013,
	title = {Active properties of neocortical pyramidal neuron dendrites},
	volume = {36},
	issn = {1545-4126},
	doi = {10.1146/annurev-neuro-062111-150343},
	abstract = {Dendrites are the main recipients of synaptic inputs and are important sites that determine neurons' input-output functions. This review focuses on thin neocortical dendrites, which receive the vast majority of synaptic inputs in cortex but also have specialized electrogenic properties. We present a simplified working-model biophysical scheme of pyramidal neurons that attempts to capture the essence of their dendritic function, including the ability to behave under plausible conditions as dynamic computational subunits. We emphasize the electrogenic capabilities of NMDA receptors (NMDARs) because these transmitter-gated channels seem to provide the major nonlinear depolarizing drive in thin dendrites, even allowing full-blown NMDA spikes. We show how apparent discrepancies in experimental findings can be reconciled and discuss the current status of dendritic spikes in vivo; a dominant NMDAR contribution would indicate that the input-output relations of thin dendrites are dynamically set by network activity and cannot be fully predicted by purely reductionist approaches.},
	language = {eng},
	journal = {Annual Review of Neuroscience},
	author = {Major, Guy and Larkum, Matthew E. and Schiller, Jackie},
	month = jul,
	year = {2013},
	pmid = {23841837},
	keywords = {Action Potentials, Animals, Dendrites, Neocortex, Pyramidal Cells, Receptors, N-Methyl-D-Aspartate},
	pages = {1--24},
	file = {Active properties of neocortical pyramidal neuron dendrites:/home/jeremy/Documents/Zotero_backup/storage/GBAWSVW5/major2013.pdf.pdf:application/pdf},
}

@article{tran-van-minh_contribution_2015,
	title = {Contribution of sublinear and supralinear dendritic integration to neuronal computations},
	volume = {9},
	issn = {1662-5102},
	url = {https://www.frontiersin.org/articles/10.3389/fncel.2015.00067},
	abstract = {Nonlinear dendritic integration is thought to increase the computational ability of neurons. Most studies focus on how supralinear summation of excitatory synaptic responses arising from clustered inputs within single dendrites result in the enhancement of neuronal firing, enabling simple computations such as feature detection. Recent reports have shown that sublinear summation is also a prominent dendritic operation, extending the range of subthreshold input-output (sI/O) transformations conferred by dendrites. Like supralinear operations, sublinear dendritic operations also increase the repertoire of neuronal computations, but feature extraction requires different synaptic connectivity strategies for each of these operations. In this article we will review the experimental and theoretical findings describing the biophysical determinants of the three primary classes of dendritic operations: linear, sublinear, and supralinear. We then review a Boolean algebra-based analysis of simplified neuron models, which provides insight into how dendritic operations influence neuronal computations. We highlight how neuronal computations are critically dependent on the interplay of dendritic properties (morphology and voltage-gated channel expression), spiking threshold and distribution of synaptic inputs carrying particular sensory features. Finally, we describe how global (scattered) and local (clustered) integration strategies permit the implementation of similar classes of computations, one example being the object feature binding problem.},
	urldate = {2022-12-09},
	journal = {Frontiers in Cellular Neuroscience},
	author = {Tran-Van-Minh, Alexandra and Cazé, Romain D. and Abrahamsson, Therése and Cathala, Laurence and Gutkin, Boris S. and DiGregorio, David A.},
	year = {2015},
	file = {Full Text PDF:/home/jeremy/Documents/Zotero_backup/storage/W8NNNT9A/Tran-Van-Minh et al. - 2015 - Contribution of sublinear and supralinear dendriti.pdf:application/pdf},
}

@article{ariav_submillisecond_2003,
	title = {Submillisecond {Precision} of the {Input}-{Output} {Transformation} {Function} {Mediated} by {Fast} {Sodium} {Dendritic} {Spikes} in {Basal} {Dendrites} of {CA1} {Pyramidal} {Neurons}},
	volume = {23},
	copyright = {Copyright © 2003 Society for Neuroscience 0270-6474/03/237750-09.00/0},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/content/23/21/7750},
	doi = {10.1523/JNEUROSCI.23-21-07750.2003},
	abstract = {The ability of cortical neurons to perform temporally accurate computations has been shown to be important for encoding of information in the cortex; however, cortical neurons are expected to be imprecise temporal encoders because of the stochastic nature of synaptic transmission and ion channel gating, dendritic filtering, and background synaptic noise. Here we show for the first time that fast local spikes in basal dendrites can serve to improve the temporal precision of neuronal output. Integration of coactivated, spatially distributed synaptic inputs produces temporally imprecise output action potentials within a time window of several milliseconds. In contrast, integration of closely spaced basal inputs initiates local dendritic spikes that amplify and sharpen the summed somatic potential. In turn, these fast basal spikes allow precise timing of output action potentials with submillisecond temporal jitter over a wide range of activation intensities and background synaptic noise. Our findings indicate that fast spikes initiated in individual basal dendrites can serve as precise “timers” of output action potentials in various network activity states and thus may contribute to temporal coding in the cortex.},
	language = {en},
	number = {21},
	urldate = {2022-12-09},
	journal = {Journal of Neuroscience},
	author = {Ariav, Gal and Polsky, Alon and Schiller, Jackie},
	month = aug,
	year = {2003},
	pmid = {12944503},
	note = {Publisher: Society for Neuroscience
Section: Cellular/Molecular},
	keywords = {cortex, dendrites, spike, submillisecond, synaptic integration, temporal coding},
	pages = {7750--7758},
	file = {Full Text PDF:/home/jeremy/Documents/Zotero_backup/storage/SCSQJZGR/Ariav et al. - 2003 - Submillisecond Precision of the Input-Output Trans.pdf:application/pdf;Submillisecond Precision of the Input-Output Transformation Function Mediated by Fast Sodium Dendritic Spikes in Basal Dendrites of CA1 Pyramidal Neurons:/home/jeremy/Documents/Zotero_backup/storage/SFXVCBB4/ariav2003.pdf.pdf:application/pdf},
}

@article{iyer_avoiding_2022,
	title = {Avoiding {Catastrophe}: {Active} {Dendrites} {Enable} {Multi}-{Task} {Learning} in {Dynamic} {Environments}},
	volume = {16},
	issn = {1662-5218},
	shorttitle = {Avoiding {Catastrophe}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9100780/},
	doi = {10.3389/fnbot.2022.846219},
	abstract = {A key challenge for AI is to build embodied systems that operate in dynamically changing environments. Such systems must adapt to changing task contexts and learn continuously. Although standard deep learning systems achieve state of the art results on static benchmarks, they often struggle in dynamic scenarios. In these settings, error signals from multiple contexts can interfere with one another, ultimately leading to a phenomenon known as catastrophic forgetting. In this article we investigate biologically inspired architectures as solutions to these problems. Specifically, we show that the biophysical properties of dendrites and local inhibitory systems enable networks to dynamically restrict and route information in a context-specific manner. Our key contributions are as follows: first, we propose a novel artificial neural network architecture that incorporates active dendrites and sparse representations into the standard deep learning framework. Next, we study the performance of this architecture on two separate benchmarks requiring task-based adaptation: Meta-World, a multi-task reinforcement learning environment where a robotic agent must learn to solve a variety of manipulation tasks simultaneously; and a continual learning benchmark in which the model's prediction task changes throughout training. Analysis on both benchmarks demonstrates the emergence of overlapping but distinct and sparse subnetworks, allowing the system to fluidly learn multiple tasks with minimal forgetting. Our neural implementation marks the first time a single architecture has achieved competitive results in both multi-task and continual learning settings. Our research sheds light on how biological properties of neurons can inform deep learning systems to address dynamic scenarios that are typically impossible for traditional ANNs to solve.},
	urldate = {2022-06-23},
	journal = {Frontiers in Neurorobotics},
	author = {Iyer, Abhiram and Grewal, Karan and Velu, Akash and Souza, Lucas Oliveira and Forest, Jeremy and Ahmad, Subutai},
	month = apr,
	year = {2022},
	pmid = {35574225},
	pmcid = {PMC9100780},
	note = {00001 },
	pages = {846219},
	file = {Iyer et al_2022_Avoiding Catastrophe.pdf:/home/jeremy/Documents/Zotero_backup/storage/3LAFA9MD/Iyer et al_2022_Avoiding Catastrophe.pdf:application/pdf},
}

@misc{jones_can_2020,
	title = {Can {Single} {Neurons} {Solve} {MNIST}? {The} {Computational} {Power} of {Biological} {Dendritic} {Trees}},
	shorttitle = {Can {Single} {Neurons} {Solve} {MNIST}?},
	url = {http://arxiv.org/abs/2009.01269},
	doi = {10.48550/arXiv.2009.01269},
	abstract = {Physiological experiments have highlighted how the dendrites of biological neurons can nonlinearly process distributed synaptic inputs. This is in stark contrast to units in artificial neural networks that are generally linear apart from an output nonlinearity. If dendritic trees can be nonlinear, biological neurons may have far more computational power than their artificial counterparts. Here we use a simple model where the dendrite is implemented as a sequence of thresholded linear units. We find that such dendrites can readily solve machine learning problems, such as MNIST or CIFAR-10, and that they benefit from having the same input onto several branches of the dendritic tree. This dendrite model is a special case of sparse network. This work suggests that popular neuron models may severely underestimate the computational power enabled by the biological fact of nonlinear dendrites and multiple synapses per pair of neurons. The next generation of artificial neural networks may significantly benefit from these biologically inspired dendritic architectures.},
	urldate = {2023-10-18},
	publisher = {arXiv},
	author = {Jones, Ilenna Simone and Kording, Konrad Paul},
	month = sep,
	year = {2020},
	note = {arXiv:2009.01269 [q-bio]},
	keywords = {Quantitative Biology - Neurons and Cognition},
	file = {arXiv Fulltext PDF:/home/jeremy/Documents/Zotero_backup/storage/JANWRWJK/Jones and Kording - 2020 - Can Single Neurons Solve MNIST The Computational .pdf:application/pdf;arXiv.org Snapshot:/home/jeremy/Documents/Zotero_backup/storage/P79MNXBD/2009.html:text/html},
}

@article{perez-nieves_neural_2021,
	title = {Neural heterogeneity promotes robust learning},
	volume = {12},
	copyright = {2021 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-26022-3},
	doi = {10.1038/s41467-021-26022-3},
	abstract = {The brain is a hugely diverse, heterogeneous structure. Whether or not heterogeneity at the neural level plays a functional role remains unclear, and has been relatively little explored in models which are often highly homogeneous. We compared the performance of spiking neural networks trained to carry out tasks of real-world difficulty, with varying degrees of heterogeneity, and found that heterogeneity substantially improved task performance. Learning with heterogeneity was more stable and robust, particularly for tasks with a rich temporal structure. In addition, the distribution of neuronal parameters in the trained networks is similar to those observed experimentally. We suggest that the heterogeneity observed in the brain may be more than just the byproduct of noisy processes, but rather may serve an active and important role in allowing animals to learn in changing environments.},
	language = {en},
	number = {1},
	urldate = {2023-10-19},
	journal = {Nature Communications},
	author = {Perez-Nieves, Nicolas and Leung, Vincent C. H. and Dragotti, Pier Luigi and Goodman, Dan F. M.},
	month = oct,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Electrical and electronic engineering, Learning algorithms, Network models},
	pages = {5791},
	file = {Full Text PDF:/home/jeremy/Documents/Zotero_backup/storage/EPT26AE6/Perez-Nieves et al. - 2021 - Neural heterogeneity promotes robust learning.pdf:application/pdf},
}

@article{deng_rethinking_2020,
	title = {Rethinking the performance comparison between {SNNS} and {ANNS}},
	volume = {121},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608019302667},
	doi = {10.1016/j.neunet.2019.09.005},
	abstract = {Artificial neural networks (ANNs), a popular path towards artificial intelligence, have experienced remarkable success via mature models, various benchmarks, open-source datasets, and powerful computing platforms. Spiking neural networks (SNNs), a category of promising models to mimic the neuronal dynamics of the brain, have gained much attention for brain inspired computing and been widely deployed on neuromorphic devices. However, for a long time, there are ongoing debates and skepticisms about the value of SNNs in practical applications. Except for the low power attribute benefit from the spike-driven processing, SNNs usually perform worse than ANNs especially in terms of the application accuracy. Recently, researchers attempt to address this issue by borrowing learning methodologies from ANNs, such as backpropagation, to train high-accuracy SNN models. The rapid progress in this domain continuously produces amazing results with ever-increasing network size, whose growing path seems similar to the development of deep learning. Although these ways endow SNNs the capability to approach the accuracy of ANNs, the natural superiorities of SNNs and the way to outperform ANNs are potentially lost due to the use of ANN-oriented workloads and simplistic evaluation metrics. In this paper, we take the visual recognition task as a case study to answer the questions of “what workloads are ideal for SNNs and how to evaluate SNNs makes sense”. We design a series of contrast tests using different types of datasets (ANN-oriented and SNN-oriented), diverse processing models, signal conversion methods, and learning algorithms. We propose comprehensive metrics on the application accuracy and the cost of memory \& compute to evaluate these models, and conduct extensive experiments. We evidence the fact that on ANN-oriented workloads, SNNs fail to beat their ANN counterparts; while on SNN-oriented workloads, SNNs can fully perform better. We further demonstrate that in SNNs there exists a trade-off between the application accuracy and the execution cost, which will be affected by the simulation time window and firing threshold. Based on these abundant analyses, we recommend the most suitable model for each scenario. To the best of our knowledge, this is the first work using systematical comparisons to explicitly reveal that the straightforward workload porting from ANNs to SNNs is unwise although many works are doing so and a comprehensive evaluation indeed matters. Finally, we highlight the urgent need to build a benchmarking framework for SNNs with broader tasks, datasets, and metrics.},
	urldate = {2023-10-19},
	journal = {Neural Networks},
	author = {Deng, Lei and Wu, Yujie and Hu, Xing and Liang, Ling and Ding, Yufei and Li, Guoqi and Zhao, Guangshe and Li, Peng and Xie, Yuan},
	month = jan,
	year = {2020},
	keywords = {Artificial neural networks, Benchmark, Deep learning, Neuromorphic computing, Spiking neural networks},
	pages = {294--307},
	file = {Rethinking the performance comparison between SNNS and ANNS:/home/jeremy/Documents/Zotero_backup/storage/J66IXPIQ/deng2019.pdf.pdf:application/pdf;ScienceDirect Snapshot:/home/jeremy/Documents/Zotero_backup/storage/WLV6E33I/S0893608019302667.html:text/html},
}

@article{payeur_burst-dependent_2021,
	title = {Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits},
	volume = {24},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/s41593-021-00857-x},
	doi = {10.1038/s41593-021-00857-x},
	abstract = {Synaptic plasticity is believed to be a key physiological mechanism for learning. It is well established that it depends on pre- and postsynaptic activity. However, models that rely solely on pre- and postsynaptic activity for synaptic changes have, so far, not been able to account for learning complex tasks that demand credit assignment in hierarchical networks. Here we show that if synaptic plasticity is regulated by high-frequency bursts of spikes, then pyramidal neurons higher in a hierarchical circuit can coordinate the plasticity of lower-level connections. Using simulations and mathematical analyses, we demonstrate that, when paired with short-term synaptic dynamics, regenerative activity in the apical dendrites and synaptic plasticity in feedback pathways, a burst-dependent learning rule can solve challenging tasks that require deep network architectures. Our results demonstrate that well-known properties of dendrites, synapses and synaptic plasticity are sufficient to enable sophisticated learning in hierarchical circuits.},
	language = {en},
	number = {7},
	urldate = {2023-10-19},
	journal = {Nature Neuroscience},
	author = {Payeur, Alexandre and Guerguiev, Jordan and Zenke, Friedemann and Richards, Blake A. and Naud, Richard},
	month = jul,
	year = {2021},
	note = {Number: 7
Publisher: Nature Publishing Group},
	keywords = {Learning algorithms, Sensory processing, Spike-timing-dependent plasticity},
	pages = {1010--1019},
	file = {Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits:/home/jeremy/Documents/Zotero_backup/storage/W6J4A865/payeur2021.pdf.pdf:application/pdf;Submitted Version:/home/jeremy/Documents/Zotero_backup/storage/4VMH74RF/Payeur et al. - 2021 - Burst-dependent synaptic plasticity can coordinate.pdf:application/pdf},
}

@article{zeldenrust_efficient_2021,
	title = {Efficient and robust coding in heterogeneous recurrent networks},
	volume = {17},
	issn = {1553-7358},
	doi = {10.1371/journal.pcbi.1008673},
	abstract = {Cortical networks show a large heterogeneity of neuronal properties. However, traditional coding models have focused on homogeneous populations of excitatory and inhibitory neurons. Here, we analytically derive a class of recurrent networks of spiking neurons that close to optimally track a continuously varying input online, based on two assumptions: 1) every spike is decoded linearly and 2) the network aims to reduce the mean-squared error between the input and the estimate. From this we derive a class of predictive coding networks, that unifies encoding and decoding and in which we can investigate the difference between homogeneous networks and heterogeneous networks, in which each neurons represents different features and has different spike-generating properties. We find that in this framework, 'type 1' and 'type 2' neurons arise naturally and networks consisting of a heterogeneous population of different neuron types are both more efficient and more robust against correlated noise. We make two experimental predictions: 1) we predict that integrators show strong correlations with other integrators and resonators are correlated with resonators, whereas the correlations are much weaker between neurons with different coding properties and 2) that 'type 2' neurons are more coherent with the overall network activity than 'type 1' neurons.},
	language = {eng},
	number = {4},
	journal = {PLoS computational biology},
	author = {Zeldenrust, Fleur and Gutkin, Boris and Denéve, Sophie},
	month = apr,
	year = {2021},
	pmid = {33930016},
	pmcid = {PMC8115785},
	keywords = {Models, Neurological, Nerve Net, Neural Networks, Computer, Neurons},
	pages = {e1008673},
	file = {Full Text:/home/jeremy/Documents/Zotero_backup/storage/JUHQ7HAI/Zeldenrust et al. - 2021 - Efficient and robust coding in heterogeneous recur.pdf:application/pdf},
}

@article{donohue_comparative_2008,
	title = {A {Comparative} {Computer} {Simulation} of {Dendritic} {Morphology}},
	volume = {4},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000089},
	doi = {10.1371/journal.pcbi.1000089},
	abstract = {Computational modeling of neuronal morphology is a powerful tool for understanding developmental processes and structure-function relationships. We present a multifaceted approach based on stochastic sampling of morphological measures from digital reconstructions of real cells. We examined how dendritic elongation, branching, and taper are controlled by three morphometric determinants: Branch Order, Radius, and Path Distance from the soma. Virtual dendrites were simulated starting from 3,715 neuronal trees reconstructed in 16 different laboratories, including morphological classes as diverse as spinal motoneurons and dentate granule cells. Several emergent morphometrics were used to compare real and virtual trees. Relating model parameters to Branch Order best constrained the number of terminations for most morphological classes, except pyramidal cell apical trees, which were better described by a dependence on Path Distance. In contrast, bifurcation asymmetry was best constrained by Radius for apical, but Path Distance for basal trees. All determinants showed similar performance in capturing total surface area, while surface area asymmetry was best determined by Path Distance. Grouping by other characteristics, such as size, asymmetry, arborizations, or animal species, showed smaller differences than observed between apical and basal, pointing to the biological importance of this separation. Hybrid models using combinations of the determinants confirmed these trends and allowed a detailed characterization of morphological relations. The differential findings between morphological groups suggest different underlying developmental mechanisms. By comparing the effects of several morphometric determinants on the simulation of different neuronal classes, this approach sheds light on possible growth mechanism variations responsible for the observed neuronal diversity.},
	language = {en},
	number = {6},
	urldate = {2023-10-19},
	journal = {PLOS Computational Biology},
	author = {Donohue, Duncan E. and Ascoli, Giorgio A.},
	month = jun,
	year = {2008},
	note = {Publisher: Public Library of Science},
	keywords = {Cell cycle and cell division, Dendritic structure, Morphometry, Neuronal dendrites, Neuronal morphology, Pyramidal cells, Radii, Statistical distributions},
	pages = {e1000089},
	file = {A Comparative Computer Simulation of Dendritic Morphology:/home/jeremy/Documents/Zotero_backup/storage/ATTPUXCV/donohue2008.pdf.pdf:application/pdf;Full Text PDF:/home/jeremy/Documents/Zotero_backup/storage/IFUKH5WL/Donohue and Ascoli - 2008 - A Comparative Computer Simulation of Dendritic Mor.pdf:application/pdf},
}

@article{beniaguev_single_2021,
	title = {Single cortical neurons as deep artificial neural networks},
	volume = {109},
	issn = {1097-4199},
	doi = {10.1016/j.neuron.2021.07.002},
	abstract = {Utilizing recent advances in machine learning, we introduce a systematic approach to characterize neurons' input/output (I/O) mapping complexity. Deep neural networks (DNNs) were trained to faithfully replicate the I/O function of various biophysical models of cortical neurons at millisecond (spiking) resolution. A temporally convolutional DNN with five to eight layers was required to capture the I/O mapping of a realistic model of a layer 5 cortical pyramidal cell (L5PC). This DNN generalized well when presented with inputs widely outside the training distribution. When NMDA receptors were removed, a much simpler network (fully connected neural network with one hidden layer) was sufficient to fit the model. Analysis of the DNNs' weight matrices revealed that synaptic integration in dendritic branches could be conceptualized as pattern matching from a set of spatiotemporal templates. This study provides a unified characterization of the computational complexity of single neurons and suggests that cortical networks therefore have a unique architecture, potentially supporting their computational power.},
	language = {eng},
	number = {17},
	journal = {Neuron},
	author = {Beniaguev, David and Segev, Idan and London, Michael},
	month = sep,
	year = {2021},
	pmid = {34380016},
	keywords = {calcium spike, Cerebral Cortex, compartmental model, cortical pyramidal neuron, deep learning, Deep Learning, Dendrites, dendritic computation, dendritic nonlinearities, Humans, machine learning, Models, Neurological, neural coding, NMDA spike, Pyramidal Cells, Receptors, N-Methyl-D-Aspartate, synaptic integration},
	pages = {2727--2739.e3},
	file = {Full Text:/home/jeremy/Documents/Zotero_backup/storage/DNSER45P/Beniaguev et al. - 2021 - Single cortical neurons as deep artificial neural .pdf:application/pdf;Single cortical neurons as deep artificial neural networks:/home/jeremy/Documents/Zotero_backup/storage/KPDATA9C/S0896627321005018.pdf.pdf:application/pdf},
}
